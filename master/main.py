# generated by fastapi-codegen:
#   filename:  client-master_openapi.json
#   timestamp: 2025-09-17T03:29:58+00:00

from __future__ import annotations

import argparse
import asyncio
import uuid
import time
import requests
import os
from enum import Enum
from typing import Dict, List, Optional
from fastapi import BackgroundTasks, FastAPI, Request, status, HTTPException
from fastapi.responses import JSONResponse
from pathlib import Path
from urllib.parse import urlparse
from .models import (
    HealthCheck,
    JobCancelJobIdPostResponse,
    JobResultJobIdGetResponse,
    JobStatusJobIdGetResponse,
    JobSubmitPostRequest,
    JobSubmitPostResponse,
    RegisterWorkerRequest,
    RegisterWorkerResponse,
)

from mapreduce.types import MapTask, ReduceTask, TaskResult, TaskStatus, TaskType


def handle_master(args: argparse.Namespace) -> None:
    import uvicorn

    # Configure NFS settings for the master instance
    if hasattr(args, "use_nfs") and args.use_nfs:
        master_instance.configure_nfs(args.nfs_server_path)

    config = uvicorn.Config(app=app, host="0.0.0.0", port=args.port, log_level="info")
    server = uvicorn.Server(config)
    asyncio.run(server.serve())


class STATUS(str, Enum):
    STARTED = "STARTED"
    RUNNING = "RUNNING"
    STOPPED = "STOPPED"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    INACTIVE = "DONE"
    BUSY = "BUSY"
    AVAILABLE = "AVAILABLE"


class WorkerInfo:
    def __init__(self, worker_id: str, worker_url: str, worker_type: str):
        self.worker_id = worker_id
        self.worker_url = worker_url
        self.worker_type = worker_type
        self.status = STATUS.AVAILABLE
        self.last_heartbeat = time.time()
        self.current_tasks: List[str] = []


class JobTracker:
    """Tracks and orchestrates MapReduce jobs"""

    def __init__(
        self, job_id: str, input_files: List[str], output_dir: str, code_url: str
    ):
        self.job_id = job_id
        self.input_files = input_files
        self.output_dir = output_dir
        self.code_url = code_url
        self.status = STATUS.STARTED
        self.map_tasks: Dict[str, MapTask] = {}
        self.reduce_tasks: Dict[str, ReduceTask] = {}
        self.completed_map_tasks: Dict[str, TaskResult] = {}
        self.completed_reduce_tasks: Dict[str, TaskResult] = {}
        self.assigned_tasks: Dict[str, str] = {}  # task_id -> worker_id
        self.progress = 0.0
        self.error_message: Optional[str] = None


class Master:
    def __init__(self):
        self.jobs: Dict[str, JobTracker] = {}
        self.workers: Dict[str, WorkerInfo] = {}
        self.use_nfs = False
        self.nfs_server_path = "/shared/gridmr"
        self.nfs_input_path = None
        self.nfs_jobs_path = None

    def configure_nfs(self, nfs_server_path: str):
        """Configure NFS paths for distributed storage"""
        self.use_nfs = True
        self.nfs_server_path = nfs_server_path
        self.nfs_input_path = Path(nfs_server_path) / "input"
        self.nfs_jobs_path = Path(nfs_server_path) / "jobs"

        # Ensure NFS directories exist on the server
        self.nfs_input_path.mkdir(parents=True, exist_ok=True)
        self.nfs_jobs_path.mkdir(parents=True, exist_ok=True)

        print(f"‚úÖ NFS configured - Server path: {self.nfs_server_path}")
        print(f"   Input path: {self.nfs_input_path}")
        print(f"   Jobs path: {self.nfs_jobs_path}")

    def resolve_path(self, data_url):
        """Resolve path with NFS support"""
        file_url = urlparse(str(data_url))
        host = file_url.netloc or "localhost"
        raw_path = file_url.path

        if host in ("localhost", "127.0.0.1"):
            if self.use_nfs:
                # For NFS mode, convert local paths to NFS server paths
                if raw_path.startswith("/"):
                    # Check if it's already pointing to NFS input directory
                    if str(self.nfs_input_path) in raw_path:
                        return Path(raw_path)
                    else:
                        # Map to NFS input directory
                        # Extract just the directory name from the path
                        dir_name = Path(raw_path).name
                        nfs_input_dir = self.nfs_input_path / dir_name
                        if nfs_input_dir.exists():
                            return nfs_input_dir
                        else:
                            # Fall back to original path resolution
                            return Path(raw_path)
                else:
                    # Relative path - resolve against NFS input directory
                    return self.nfs_input_path / raw_path
            else:
                # Original local mode
                if raw_path.startswith("/"):
                    return Path(raw_path)
                else:
                    return Path(raw_path).resolve()
        else:
            raise ValueError(f"Remote host {host} not supported yet")

    def create_job(self, data_url: str, code_url: str, job_name: str = None) -> str:
        # Generate shorter, cleaner job ID (8 characters instead of full UUID)
        job_id = str(uuid.uuid4()).split("-")[0]

        # Resolve input files
        input_dir = self.resolve_path(data_url)
        input_files = [str(f) for f in input_dir.iterdir() if f.is_file()]

        # Create output directory structure based on NFS configuration
        if self.use_nfs:
            # NFS mode: jobs/{job_id}
            output_dir = str(self.nfs_jobs_path / job_id)
        else:
            # Local mode: output/{job_id}
            base_output_dir = Path.cwd() / "output"
            output_dir = str(base_output_dir / job_id)

        job_tracker = JobTracker(job_id, input_files, output_dir, code_url)
        self.jobs[job_id] = job_tracker

        print(f"üöÄ Created job {job_id}")
        print(f"   Input files: {len(input_files)} files from {input_dir}")
        print(f"   Output directory: {output_dir}")
        print(f"   Storage mode: {'NFS' if self.use_nfs else 'Local'}")

        return job_id

    def register_worker(self, worker_id: str, worker_url: str, worker_type: str) -> str:
        if worker_id in self.workers:
            return "worker already exists"

        worker_info = WorkerInfo(worker_id, worker_url, worker_type)
        self.workers[worker_id] = worker_info
        print(f"Registered worker {worker_id} at {worker_url}")

        return "successfully created"

    def update_worker_heartbeat(
        self, worker_id: str, status: str, current_tasks: List[dict]
    ):
        """Update worker heartbeat and status"""
        if worker_id in self.workers:
            worker = self.workers[worker_id]
            worker.last_heartbeat = time.time()
            worker.status = status
            worker.current_tasks = [task.get("task_id", "") for task in current_tasks]

    def get_available_workers(self) -> List[WorkerInfo]:
        """Get list of available workers"""
        current_time = time.time()
        available = []

        print("üîç Checking worker availability...")
        print(f"   Total registered workers: {len(self.workers)}")

        for worker in self.workers.values():
            time_since_heartbeat = current_time - worker.last_heartbeat
            is_available = worker.status == STATUS.AVAILABLE
            is_not_timed_out = time_since_heartbeat < 300

            print(f"   Worker {worker.worker_id}:")
            print(f"     URL: {worker.worker_url}")
            print(f"     Status: {worker.status}")
            print(f"     Last heartbeat: {time_since_heartbeat:.1f}s ago")
            print(f"     Available: {is_available}, Not timed out: {is_not_timed_out}")

            # Check if worker is available and hasn't timed out (5 minutes)
            if is_available and is_not_timed_out:
                available.append(worker)
                print("     ‚úÖ Added to available workers")
            else:
                print("     ‚ùå Not available")

        print(f"üéØ Available workers: {len(available)} out of {len(self.workers)}")
        return available

    async def assign_task_to_worker(
        self, task: MapTask | ReduceTask, worker: WorkerInfo
    ) -> bool:
        """Assign a task to a specific worker"""
        print(
            f"üîÑ Attempting to assign task {task.task_id} to worker {worker.worker_id} at {worker.worker_url}"
        )

        try:
            task_data = {
                "task_type": "MAP" if isinstance(task, MapTask) else "REDUCE",
                "task_data": task.model_dump(),
            }

            print(
                f"üì§ Sending {task_data['task_type']} task to {worker.worker_url}/task/execute"
            )

            response = requests.post(
                f"{worker.worker_url}/task/execute", json=task_data, timeout=30
            )

            print(f"üì• Response from {worker.worker_id}: Status {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                print(f"‚úÖ Task response from {worker.worker_id}: {result}")

                if result["status"] != "FAILED":
                    worker.status = STATUS.BUSY
                    worker.current_tasks.append(task.task_id)
                    print(
                        f"üéØ Successfully assigned task {task.task_id} to worker {worker.worker_id}"
                    )
                    return True
                else:
                    print(
                        f"‚ùå Worker {worker.worker_id} reported task failure: {result.get('error', 'Unknown error')}"
                    )
            else:
                print(
                    f"‚ùå HTTP error from {worker.worker_id}: {response.status_code} - {response.text}"
                )

            return False

        except requests.exceptions.Timeout:
            print(
                f"‚è∞ Timeout connecting to worker {worker.worker_id} at {worker.worker_url}"
            )
            return False
        except requests.exceptions.ConnectionError as e:
            print(
                f"üîå Connection error to worker {worker.worker_id} at {worker.worker_url}: {e}"
            )
            return False
        except Exception as e:
            print(
                f"üí• Unexpected error assigning task {task.task_id} to worker {worker.worker_id}: {e}"
            )
            return False

    async def create_map_tasks(self, job_tracker: JobTracker):
        """Create and assign map tasks for a job"""
        job_tracker.status = STATUS.RUNNING

        # Create map tasks
        for i, input_file in enumerate(job_tracker.input_files):
            task_id = f"{job_tracker.job_id}_map_{i}"
            task = MapTask(
                task_id=task_id,
                input_file=input_file,
                output_dir=f"{job_tracker.output_dir}/map_output",
                mapper_code=job_tracker.code_url,
            )
            job_tracker.map_tasks[task_id] = task

        # Assign tasks to workers
        available_workers = self.get_available_workers()
        worker_index = 0

        for task_id, task in job_tracker.map_tasks.items():
            if available_workers:
                worker = available_workers[worker_index % len(available_workers)]
                success = await self.assign_task_to_worker(task, worker)

                if success:
                    job_tracker.assigned_tasks[task_id] = worker.worker_id
                    print(f"Assigned map task {task_id} to worker {worker.worker_id}")
                    worker_index += 1
                else:
                    print(f"Failed to assign map task {task_id}")
            else:
                print("No available workers for map tasks")
                job_tracker.status = STATUS.FAILED
                job_tracker.error_message = "No available workers"
                return

    async def check_map_tasks_completion(self, job_tracker: JobTracker) -> bool:
        """Check if all map tasks are completed"""
        all_completed = True

        for task_id in job_tracker.map_tasks.keys():
            if task_id not in job_tracker.completed_map_tasks:
                # Check task status with assigned worker
                worker_id = job_tracker.assigned_tasks.get(task_id)
                if worker_id and worker_id in self.workers:
                    worker = self.workers[worker_id]
                    try:
                        response = requests.get(
                            f"{worker.worker_url}/task/status/{task_id}", timeout=10
                        )

                        if response.status_code == 200:
                            task_status = response.json()
                            if task_status["status"] == "COMPLETED":
                                # Get the actual result with output files from the worker
                                if "result" in task_status and task_status["result"]:
                                    result_data = task_status["result"]
                                    output_files = result_data.get("output_files", [])
                                else:
                                    # Fallback: construct expected output file paths
                                    intermediate_dir = f"{job_tracker.output_dir}/map_output/intermediate"
                                    output_files = []
                                    import glob

                                    pattern = (
                                        f"{intermediate_dir}/map_{task_id}_part_*.txt"
                                    )
                                    output_files = glob.glob(pattern)

                                result = TaskResult(
                                    task_id=task_id,
                                    task_type=TaskType.MAP,
                                    status=TaskStatus.COMPLETED,
                                    output_files=output_files,  # Include actual output files
                                    worker_id=worker_id,
                                )
                                job_tracker.completed_map_tasks[task_id] = result
                                worker.status = STATUS.AVAILABLE
                                if task_id in worker.current_tasks:
                                    worker.current_tasks.remove(task_id)
                                print(
                                    f"Map task {task_id} completed with {len(output_files)} output files"
                                )
                            else:
                                all_completed = False
                        else:
                            all_completed = False
                    except Exception as e:
                        print(f"Error checking task {task_id} status: {e}")
                        all_completed = False
                else:
                    all_completed = False

        return all_completed

    async def create_reduce_tasks(self, job_tracker: JobTracker):
        """Create and assign reduce tasks after map phase completes"""
        # Collect actual intermediate files from completed map tasks
        intermediate_files = []
        for task_result in job_tracker.completed_map_tasks.values():
            intermediate_files.extend(task_result.output_files)

        print(
            f"DEBUG: Found {len(intermediate_files)} intermediate files from map tasks"
        )
        for file_path in intermediate_files:
            print(f"DEBUG: Intermediate file: {file_path}")

        # Group files by partition ID
        partitions: Dict[int, List[str]] = {}
        for file_path in intermediate_files:
            if os.path.exists(file_path):
                # Extract partition ID from filename (e.g., map_task_id_part_0.txt)
                filename = os.path.basename(file_path)
                if "_part_" in filename:
                    try:
                        partition_id = int(filename.split("_part_")[1].split(".")[0])
                        if partition_id not in partitions:
                            partitions[partition_id] = []
                        partitions[partition_id].append(file_path)
                        print(f"DEBUG: Assigned {filename} to partition {partition_id}")
                    except (ValueError, IndexError):
                        print(
                            f"Warning: Could not parse partition from filename: {filename}"
                        )
            else:
                print(f"Warning: Intermediate file does not exist: {file_path}")

        print(f"DEBUG: Created {len(partitions)} partitions: {list(partitions.keys())}")

        # Create reduce task for each partition that has files
        for partition_id, files in partitions.items():
            task_id = f"{job_tracker.job_id}_reduce_{partition_id}"

            # Ensure output directory exists
            os.makedirs(f"{job_tracker.output_dir}/reduce_output", exist_ok=True)

            task = ReduceTask(
                task_id=task_id,
                input_files=files,
                output_file=f"{job_tracker.output_dir}/reduce_output/part-{partition_id:05d}.txt",
                reducer_code=job_tracker.code_url,
                partition_id=partition_id,
            )
            job_tracker.reduce_tasks[task_id] = task
            print(f"DEBUG: Created reduce task {task_id} with {len(files)} input files")

        print(
            f"Created {len(job_tracker.reduce_tasks)} reduce tasks for {len(intermediate_files)} intermediate files"
        )

        # Assign reduce tasks to workers
        available_workers = self.get_available_workers()
        worker_index = 0

        for task_id, task in job_tracker.reduce_tasks.items():
            if available_workers:
                worker = available_workers[worker_index % len(available_workers)]
                success = await self.assign_task_to_worker(task, worker)

                if success:
                    job_tracker.assigned_tasks[task_id] = worker.worker_id
                    print(
                        f"Assigned reduce task {task_id} to worker {worker.worker_id}"
                    )
                    worker_index += 1
                else:
                    print(f"Failed to assign reduce task {task_id}")
            else:
                print("No available workers for reduce tasks")
                job_tracker.status = STATUS.FAILED
                job_tracker.error_message = "No available workers for reduce phase"
                return

    async def check_reduce_tasks_completion(self, job_tracker: JobTracker) -> bool:
        """Check if all reduce tasks are completed"""
        all_completed = True

        for task_id in job_tracker.reduce_tasks.keys():
            if task_id not in job_tracker.completed_reduce_tasks:
                worker_id = job_tracker.assigned_tasks.get(task_id)
                if worker_id and worker_id in self.workers:
                    worker = self.workers[worker_id]
                    try:
                        response = requests.get(
                            f"{worker.worker_url}/task/status/{task_id}", timeout=10
                        )

                        if response.status_code == 200:
                            task_status = response.json()
                            if task_status["status"] == "COMPLETED":
                                result = TaskResult(
                                    task_id=task_id,
                                    task_type=TaskType.REDUCE,
                                    status=TaskStatus.COMPLETED,
                                    output_files=[],
                                    worker_id=worker_id,
                                )
                                job_tracker.completed_reduce_tasks[task_id] = result
                                worker.status = STATUS.AVAILABLE
                                if task_id in worker.current_tasks:
                                    worker.current_tasks.remove(task_id)
                            else:
                                all_completed = False
                        else:
                            all_completed = False
                    except Exception as e:
                        print(f"Error checking reduce task {task_id} status: {e}")
                        all_completed = False
                else:
                    all_completed = False

        return all_completed

    async def orchestrate_job(self, job_id: str):
        """Main job orchestration logic"""
        job_tracker = self.jobs[job_id]

        try:
            # Phase 1: Map tasks
            await self.create_map_tasks(job_tracker)

            # Wait for map tasks to complete
            while not await self.check_map_tasks_completion(job_tracker):
                job_tracker.progress = (
                    len(job_tracker.completed_map_tasks) / len(job_tracker.map_tasks)
                ) * 40  # Map phase is 40% of job
                await asyncio.sleep(5)

            print(f"Map phase completed for job {job_id}")

            # Phase 2: Reduce tasks
            await self.create_reduce_tasks(job_tracker)

            # Wait for reduce tasks to complete
            while not await self.check_reduce_tasks_completion(job_tracker):
                map_progress = 40  # Map phase completed
                reduce_progress = (
                    len(job_tracker.completed_reduce_tasks)
                    / len(job_tracker.reduce_tasks)
                ) * 40  # Reduce phase is 40%
                job_tracker.progress = map_progress + reduce_progress
                await asyncio.sleep(5)

            print(f"Reduce phase completed for job {job_id}")

            # Phase 3: Final consolidation - merge all reduce outputs into single file
            await self.consolidate_final_output(job_tracker)
            job_tracker.progress = 100.0

            # Job completed
            job_tracker.status = STATUS.COMPLETED
            print(f"Job {job_id} completed successfully")

        except Exception as e:
            job_tracker.status = STATUS.FAILED
            job_tracker.error_message = str(e)
            print(f"Job {job_id} failed: {e}")

    async def consolidate_final_output(self, job_tracker: JobTracker):
        """Consolidate all reduce outputs into a single final result file"""
        try:
            reduce_output_dir = f"{job_tracker.output_dir}/reduce_output"
            final_output_file = f"{job_tracker.output_dir}/result.txt"

            # Collect all reduce output files
            reduce_files = []
            for partition_file in os.listdir(reduce_output_dir):
                if partition_file.startswith("part-") and partition_file.endswith(
                    ".txt"
                ):
                    reduce_files.append(os.path.join(reduce_output_dir, partition_file))

            # Sort files to ensure consistent ordering
            reduce_files.sort()

            # Merge all reduce outputs into single file, sorted by key
            all_results = []
            for reduce_file in reduce_files:
                with open(reduce_file, "r") as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split("\t", 1)
                            if len(parts) == 2:
                                key, value = parts
                                all_results.append((key, value))

            # Sort final results by key for consistent output
            all_results.sort(key=lambda x: x[0])

            # Write consolidated results
            with open(final_output_file, "w") as f:
                for key, value in all_results:
                    f.write(f"{key}\t{value}\n")

            print(f"Consolidated {len(all_results)} results into {final_output_file}")

        except Exception as e:
            print(f"Error consolidating final output: {e}")
            raise


master_instance = Master()
app = FastAPI(
    title="MapReduce Master API",
    version="0.2.0",
)


@app.post("/worker/register")
async def register_worker(request: Request, body: RegisterWorkerRequest):
    """
    Register a worker machine.
    """
    client_host = (
        getattr(request.client, "host", "unknown") if request.client else "unknown"
    )
    client_port = (
        getattr(request.client, "port", "unknown") if request.client else "unknown"
    )

    print(f"üîó Worker registration request from {client_host}:{client_port}")
    print(f"   Request body: {body}")
    print(f"   Request headers: {dict(request.headers)}")

    # Extract worker info from request headers or generate ID
    worker_id = request.headers.get("X-Worker-ID", f"worker_{uuid.uuid4().hex[:8]}")

    # CRITICAL FIX for AWS: Use the client's public IP + their advertised port
    if "X-Worker-Port" in request.headers:
        # Worker sent their port, construct URL using their public IP
        worker_port = request.headers.get("X-Worker-Port")
        worker_url = f"http://{client_host}:{worker_port}"
        print(f"üåê AWS mode: Using worker's public IP {client_host}:{worker_port}")
    else:
        # Fallback to old method
        worker_url = request.headers.get("X-Worker-URL", f"http://{client_host}:8001")
        print(f"üè† Local mode: Using provided/default URL {worker_url}")

    print("üÜî Registering worker:")
    print(f"   Worker ID: {worker_id}")
    print(f"   Worker URL: {worker_url}")
    print(f"   Worker Type: {body.worker_type}")

    response = master_instance.register_worker(worker_id, worker_url, body.worker_type)

    print(f"‚úÖ Registration response: {response}")

    return JSONResponse(
        content=RegisterWorkerResponse(
            worker_url=worker_url, worker_status=response
        ).model_dump()
    )


@app.post("/worker/heartbeat")
async def worker_heartbeat(heartbeat_data: dict):
    """
    Receive heartbeat from worker
    """
    worker_id = heartbeat_data.get("worker_id")
    status = heartbeat_data.get("status", "AVAILABLE")
    current_tasks = heartbeat_data.get("current_tasks", [])

    master_instance.update_worker_heartbeat(worker_id, status, current_tasks)

    return {"status": "ok"}


@app.get(
    "/",
    response_model=HealthCheck,
)
def health_check() -> HealthCheck:
    """
    Health Check
    """
    return HealthCheck(status="ok")


@app.post("/job/cancel/{job_id}", response_model=JobCancelJobIdPostResponse)
def cancel_job(job_id: str) -> JobCancelJobIdPostResponse:
    """
    Cancel a running job
    """
    if job_id in master_instance.jobs:
        job_tracker = master_instance.jobs[job_id]
        job_tracker.status = STATUS.STOPPED
        return JobCancelJobIdPostResponse(job_id=job_id, status="cancelled")
    else:
        raise HTTPException(status_code=404, detail="Job not found")


@app.get("/job/result/{job_id}", response_model=JobResultJobIdGetResponse)
def get_job_result(job_id: str) -> JobResultJobIdGetResponse:
    """
    Get job result
    """
    if job_id not in master_instance.jobs:
        raise HTTPException(status_code=404, detail="Job not found")

    job_tracker = master_instance.jobs[job_id]

    if job_tracker.status == STATUS.COMPLETED:
        result_url = f"file://{job_tracker.output_dir}/reduce_output/"
        return JobResultJobIdGetResponse(job_id=job_id, result_url=result_url)
    elif job_tracker.status in [STATUS.RUNNING, STATUS.STARTED]:
        raise HTTPException(status_code=202, detail="Job still running")
    else:
        raise HTTPException(status_code=404, detail="Job result not available")


@app.get("/job/status/{job_id}", response_model=JobStatusJobIdGetResponse)
def get_job_status(job_id: str) -> JobStatusJobIdGetResponse:
    """
    Get job status
    """
    if job_id not in master_instance.jobs:
        raise HTTPException(status_code=404, detail="Job not found")

    job_tracker = master_instance.jobs[job_id]

    # Map internal status to API status
    api_status = job_tracker.status.lower()
    if api_status == "started":
        api_status = "submitted"
    elif api_status == "stopped":
        api_status = "failed"

    return JobStatusJobIdGetResponse(
        job_id=job_id, status=api_status, progress=job_tracker.progress
    )


@app.post(
    "/job/submit",
    response_model=None,
    responses={
        "201": {
            "model": JobSubmitPostResponse,
            "description": "Job accepted",
        },
        "400": {"description": "Invalid job submission"},
    },
)
def submit_job(
    body: JobSubmitPostRequest, background_tasks: BackgroundTasks
) -> JSONResponse:
    """
    Submit a new job
    """
    try:
        job_id = master_instance.create_job(
            data_url=str(body.data_url),
            code_url=str(body.code_url),
            job_name=body.job_name,
        )

        # Start job orchestration in background
        background_tasks.add_task(master_instance.orchestrate_job, job_id)

        return JSONResponse(
            content=JobSubmitPostResponse(
                job_id=job_id,
                status="Job started successfully",
            ).model_dump(),
            status_code=status.HTTP_201_CREATED,
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
